{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5bf2d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T10:58:57.374314Z",
     "start_time": "2024-03-20T10:58:53.162501Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import wget\n",
    "import random\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3da18",
   "metadata": {},
   "source": [
    "Установим утилиту для лемматизации, зададим конфигурацию корпусу стоп-слов в русском языке и установим обработку модели на GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2cf2bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T10:58:57.722158Z",
     "start_time": "2024-03-20T10:58:57.399752Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "udpipe_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "modelfile = 'udpipe_syntagrus.model'\n",
    "if not os.path.isfile(modelfile):\n",
    "        wget.download(udpipe_url)\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "sw = stopwords.words(\"russian\")\n",
    "with open('textdata/swm.txt', 'r', encoding='UTF-8') as swm_file:\n",
    "    swm = swm_file.read().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6378348",
   "metadata": {},
   "source": [
    "Переформатируем текст так, чтобы в нём остались только предложения на русском"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53dd5bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T10:58:57.738436Z",
     "start_time": "2024-03-20T10:58:57.722158Z"
    },
    "code_folding": [
     2,
     14,
     23
    ]
   },
   "outputs": [],
   "source": [
    "SENT_SEPS = ['\\.', '!', '\\?', '\\n']\n",
    "\n",
    "def apply_filters(original_text):\n",
    "    filtered_text = re.sub(\"[а-яА-ЯёЁ]*[a-zA-Z][^\\[]*\\[\", '', original_text)\n",
    "    filtered_text = re.sub(\"\\.\\]\", '', original_text)\n",
    "    filtered_text = re.sub(\"франц\\.\", '', filtered_text)\n",
    "    filtered_text = re.sub(\"нем\\.\", '', filtered_text)\n",
    "    filtered_text = re.sub(\"Ред\\.\\]\", '', filtered_text)\n",
    "    filtered_text = re.sub(\"-\", ' ', filtered_text)\n",
    "    filtered_text = re.sub(\"[^а-яА-ЯёЁ  \\-\\n.!?]\", '', filtered_text)\n",
    "    filtered_text = re.sub(\"[  ].[  ]\", ' ', filtered_text)\n",
    "    \n",
    "    return filtered_text\n",
    "    \n",
    "def get_sents(text):\n",
    "    filtered_text = ' '.join(text.split())\n",
    "    pattern = '|'.join(SENT_SEPS)\n",
    "    split_text = re.split(pattern, text)\n",
    "    sentences = [s.strip() for s in split_text if s]\n",
    "    \n",
    "    sentences = [re.sub(u'\\xa0', ' ', elem) for elem in '\\n'.join(sentences).splitlines() if len(elem)>0]\n",
    "    return sentences\n",
    "\n",
    "def get_words(text):\n",
    "    filtered_text = re.sub(\"[^а-яА-ЯёЁ\\-  \\n]\", '', text)\n",
    "    filtered_text = re.sub(\"\\n\", ' ', filtered_text)\n",
    "    filtered_text = re.sub(\"[ ]+\", ' ', filtered_text)\n",
    "    filtered_text = ' '.join(filtered_text.split())\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "944be348",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T10:58:58.584602Z",
     "start_time": "2024-03-20T10:58:57.742355Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('textdata/ViM.txt', 'r', encoding='ANSI') as original_file:\n",
    "    original_text = original_file.read()\n",
    "\n",
    "filtered_text = apply_filters(original_text)\n",
    "\n",
    "sents = get_sents(filtered_text)\n",
    "\n",
    "words = get_words('\\n'.join(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c885a",
   "metadata": {},
   "source": [
    "Зададим функции обработки предложений, из которых мы будем убирать следующие слова:\n",
    "\n",
    "\n",
    "1. Все части речи, которые несут в себе мало смысловой нагрузки (их теги определены в BANNED_TAGS).\n",
    "\n",
    "\n",
    "2. Все слова, которые содержатся в корпусе стоп-слов.\n",
    "\n",
    "\n",
    "3. Все слова длиной не более одного символа (чтобы убрать конструкции типа \"и т д\", \"т е\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2696742c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T10:59:45.406572Z",
     "start_time": "2024-03-20T10:59:45.373429Z"
    },
    "code_folding": [
     2,
     5,
     9,
     15,
     29,
     98
    ]
   },
   "outputs": [],
   "source": [
    "# BANNED_TAGS = ['_NUM', '_DET', '_CCONJ', '_SCONJ', '_PART', '_PRON']\n",
    "\n",
    "def rem_tag(word):\n",
    "    return re.sub(\"_[A-Z]*\", '', word)\n",
    "\n",
    "def num_replace(word):\n",
    "    newtoken = 'x' * len(word)\n",
    "    return newtoken\n",
    "\n",
    "def clean_token(token, misc):\n",
    "    out_token = token.strip().replace(' ', '')\n",
    "    if token == 'Файл' and 'SpaceAfter=No' in misc:\n",
    "        return None\n",
    "    return out_token\n",
    "\n",
    "def clean_lemma(lemma, pos):\n",
    "    out_lemma = lemma.strip().replace(' ', '').replace('_', '').lower()\n",
    "    if '|' in out_lemma or out_lemma.endswith('.jpg') or out_lemma.endswith('.png'):\n",
    "        return None\n",
    "    if pos != 'PUNCT':\n",
    "        if out_lemma.startswith('«') or out_lemma.startswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[1:])\n",
    "        if out_lemma.endswith('«') or out_lemma.endswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "        if out_lemma.endswith('!') or out_lemma.endswith('?') or out_lemma.endswith(',') \\\n",
    "                or out_lemma.endswith('.'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "    return out_lemma\n",
    "\n",
    "def process(pipeline, text='Строка', keep_pos=False, keep_punct=False):\n",
    "    entities = {'PROPN'}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split('\\t') for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            continue\n",
    "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        token = clean_token(token, misc)\n",
    "        lemma = clean_lemma(lemma, pos)\n",
    "        if not lemma or not token:\n",
    "            continue\n",
    "        if pos in entities:\n",
    "            if '|' not in feats:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n",
    "            if 'Case' not in morph or 'Number' not in morph:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            if not named:\n",
    "                named = True\n",
    "                mem_case = morph['Case']\n",
    "                mem_number = morph['Number']\n",
    "            if morph['Case'] == mem_case and morph['Number'] == mem_number:\n",
    "                memory.append(lemma)\n",
    "                if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n",
    "                    named = False\n",
    "                    past_lemma = '::'.join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + '_PROPN ')\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "        else:\n",
    "            if not named:\n",
    "                if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
    "                    lemma = num_replace(token)\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "\n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "    return tagged_propn\n",
    "\n",
    "\n",
    "def tag_ud(text='Строка', modelfile='udpipe_syntagrus.model'):\n",
    "    udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "    udpipe_filename = udpipe_model_url.split('/')[-1]\n",
    "\n",
    "    if not os.path.isfile(modelfile):\n",
    "        wget.download(udpipe_model_url)\n",
    "\n",
    "    model = Model.load(modelfile)\n",
    "    process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "\n",
    "    filtered = []\n",
    "    for line in text:\n",
    "        output = process(process_pipeline, text=line)\n",
    "        \n",
    "        # output = [elem for elem in output if not any(banned in elem for banned in BANNED_TAGS)]\n",
    "        \n",
    "        for i, elem in enumerate(output):\n",
    "            word = rem_tag(elem)\n",
    "            if (word in sw) or (len(word) <= 1):\n",
    "                del output[i]\n",
    "        \n",
    "        \n",
    "        filtered.append(' '.join(output))\n",
    "        \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39596c94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T11:04:58.664739Z",
     "start_time": "2024-03-20T10:59:48.956683Z"
    }
   },
   "outputs": [],
   "source": [
    "processed = tag_ud(text=sents, modelfile=modelfile)\n",
    "tokens = ' '.join(processed).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8ae164",
   "metadata": {},
   "source": [
    "Оставим только те слова, что встречаются более 5 раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8122ced",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T11:04:58.778565Z",
     "start_time": "2024-03-20T11:04:58.664739Z"
    }
   },
   "outputs": [],
   "source": [
    "word_counts = Counter(tokens)\n",
    "words = [word for word in tokens if word_counts[word] > 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f24e0",
   "metadata": {},
   "source": [
    "Сохраним обработанный текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48092781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T11:04:58.841439Z",
     "start_time": "2024-03-20T11:04:58.778565Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('textdata/words.txt', 'w', encoding='utf-8') as words_file:\n",
    "    words_file.write('\\n'.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb9502",
   "metadata": {},
   "source": [
    "Посмотрим на итоговый размер текста и словаря:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aec395e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T11:04:58.881807Z",
     "start_time": "2024-03-20T11:04:58.846634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of words: 270168\n",
      "# of unique words: 5185\n"
     ]
    }
   ],
   "source": [
    "print(\"Total # of words: {}\".format(len(words)))\n",
    "print(\"# of unique words: {}\".format(len(set(words)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabe275a",
   "metadata": {},
   "source": [
    "На этом предобработка текста завершена и можно переходить к самой модели Word2Vec."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ad",
   "language": "python",
   "name": "ad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
